# Pipeline Design - [Your Feature Name]

**Team Members:** [Your Name(s)]

---

## 1. Feature Choice

**Feature:** [Option A / Option B / Option C]

**User Story:** [One sentence describing what the user can do]

---

## 2. Pipeline Diagram

```
[Insert ASCII art or text flow diagram here]

Example:
User Query: "dog in park"
    ↓
Text Encoder (CLIP) → text_embedding (512-D)
    ↓
Similarity Search (cosine) → top_5_image_ids
    ↓
Retrieve Images from storage → [img1.jpg, img2.jpg, ...]
    ↓
Return: List of (image_url, similarity_score)
```

---

## 3. Inputs and Outputs

**Input Format:**
- Type: [e.g., PIL.Image, text string, audio file]
- Example: [e.g., "dog in park", image.jpg, audio.wav]

**Output Format:**
- Type: [e.g., List of (image_url, score), text string, dict with keys]
- Example: [e.g., `[("img1.jpg", 0.92), ("img2.jpg", 0.85), ...]`]

---

## 4. Model(s) Used

**Primary Model:**
- Name: [e.g., openai/clip-vit-base-patch32]
- Source: Hugging Face Transformers
- Task: [e.g., image-text similarity, image captioning, ASR]

**Secondary Model (if applicable):**
- Name: [e.g., none]
- Purpose: [e.g., N/A]

---

## 5. Embedding / Output Storage

**Where are embeddings or outputs stored?**
- [e.g., "In-memory NumPy array (for prototype)", "SQLite database (for production)", "Redis cache (for fast lookup)"]

**Why this choice?**
- [1-2 sentences explaining tradeoffs]

---

## 6. Success Definition

**Metric:**
- [e.g., "Top-1 accuracy on labeled test set", "BLEU score vs. ground truth captions", "Word Error Rate (WER) on transcription"]

**Threshold:**
- [e.g., "Top-1 accuracy ≥ 80% on clean images", "BLEU ≥ 0.6 on test captions", "WER ≤ 15% on clear audio"]

**Test Set Size:**
- [e.g., "100 images with ground truth labels", "50 images with human-written captions", "20 audio clips with transcripts"]

---

## 7. Three Expected Failures

### Failure 1: [Category, e.g., "Blur"]
**Description:** [e.g., "Motion-blurred image of dog running"]
**Expected Behavior:** [e.g., "Similarity scores all <0.6, no clear top-1"]
**Root Cause:** [e.g., "CLIP trained primarily on sharp images, lacks blur augmentation"]

### Failure 2: [Category, e.g., "Ambiguity"]
**Description:** [e.g., "Query 'animal' is too broad"]
**Expected Behavior:** [e.g., "Returns mix of dogs, cats, birds with similar scores"]
**Root Cause:** [e.g., "No context to disambiguate; needs follow-up question"]

### Failure 3: [Category, e.g., "Low Light"]
**Description:** [e.g., "Dark indoor image, underexposed"]
**Expected Behavior:** [e.g., "Caption hallucinates objects like 'lamp' or 'window' not present"]
**Root Cause:** [e.g., "Model trained on well-lit images, extrapolates from dark regions"]

---

## 8. Fallback Behavior

**When does fallback trigger?**
- [e.g., "If top-1 similarity score < 0.6"]

**What happens?**
- [e.g., "Show top-3 results with message: 'Low confidence. Here are closest matches. Did you mean one of these?'"]

**User Experience (exact wording):**
```
[Insert exact UX text here, e.g.:]

"⚠️ Low Confidence Match

We're not sure which image best matches your query. Here are the top 3 possibilities:

1. [Image 1] - Score: 0.58
2. [Image 2] - Score: 0.55
3. [Image 3] - Score: 0.52

Try a more specific query, like 'golden retriever in park' instead of 'dog'."
```

---

## 9. Next Steps (After This Session)

- [ ] Implement pipeline code
- [ ] Create test set with stress cases
- [ ] Log failure examples
- [ ] Write limitations statement
